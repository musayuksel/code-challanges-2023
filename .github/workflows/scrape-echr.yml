name: ECHR Scraper (Manual Run)

# This workflow only runs when YOU click the button - never automatically
on:
  workflow_dispatch:
    inputs:
      startYear:
        description: 'Start Year (e.g., 16 for 2016)'
        required: true
        default: '16'
      maxYear:
        description: 'Max Year (e.g., 16 for 2016)'
        required: true
        default: '16'
      maxSkips:
        description: 'Max consecutive skips before next year'
        required: true
        default: '500'
      startNumber:
        description: 'Starting case number'
        required: true
        default: '1'

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6 hours max - GitHub free tier limit
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
      
      - name: Install dependencies
        working-directory: ./db-old
        run: npm install
      
      - name: Install Playwright browsers
        working-directory: ./db-old
        run: npx playwright install chromium
      
      - name: Run scraper
        working-directory: ./db-old
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
        run: |
          # Update config with user inputs
          cat > scraper-config.js << EOF
          module.exports = {
            startYear: ${{ github.event.inputs.startYear }},
            maxYear: ${{ github.event.inputs.maxYear }},
            maxConsecutiveSkips: ${{ github.event.inputs.maxSkips }},
            startNumber: ${{ github.event.inputs.startNumber }}
          };
          EOF
          
          # Run the scraper
          node monthly-scraper.js
      
      - name: Upload logs (if job fails)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs
          path: db-old/*.log